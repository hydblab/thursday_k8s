# 4주차 이태화 자료

## 5. 쿠버네티스 설치

- 도커 컨테이너, 도커 스웜, 도커 컴포즈와 같은 개념을 한군데로 모아 사용할 수 있는 프로젝트: 쿠버네티스
- 주의 사항
  - 타 오케스트레이션 툴보다 다양한 지식을 필요로 함
  - 따라서 쿠버네티스 자체의 관리가 더욱 어려울 수도 있음.
  - 즉, 조직의 규모에 따라 오버 엔지니어링이 될 수 있으니, 운영/관리의 인력과 비용을 고려하여 도입 필요.

### 5.1 쿠버네티스 설치 환경의 종류

- 개발 용도: Minikube, Docker Desktop for Mac/Windows에 내장된 쿠버네티스
  - standalone 모드로 사용하기 때문에 모든 기능을 학습하기에는 어려움
- 서비스 테스트 또는 운영 용도: kops, kubespray, kubeadm, EKS(AWS), GKE(Google)
  - AWS, GKE와 같은 클라우드 플랫폼 활용
  - 또는 on-premise 환경을 활용

![쿠버네티스 설치 도구 및 서비스와 특징 비교](./assets/img1.png?raw=true)

### 5.2 쿠버네티스 버전 선택

- 버전이 관계는 없으나, 너무 최신이거나 너무 예전 버전을 사용하지 않으면 좋음

### 5.3 개발 용도의 쿠버네티스 설치

#### 5.3.1 Docker Desktop for Mac / Windows에서 쿠버네티스 사용

- Kubenernetes 탭에서 활성화

![Docker Desktop for Mac에서 쿠버네티스 활성화](./assets/img2.png?raw=true)

#### 5.3.2 Minikube로 쿠버네티스 설치

- 앞서 설명한 대로, 기능을 간단히 사용해볼 수는 있지만, 실제 운영 환경에서는 Minikube를 적용하기 힘듬
- 가능하다면 여러 대의 서버로 쿠버네티스 클러스터를 구성하는 것이 좋음

##### 기본 설정을 이용해 버추어 박스로 minikube 설치

- 생략

##### 리눅스 서버에서 가상 머신 없이 도커 엔진만으로 minikube 설치

- 참고: <https://minikube.sigs.k8s.io/docs/start/>
- Minikube 설치 (root 사용자로 실행하기 위해서는 --force를 활성화 해야 함)

```bash
[root@lahdev ~]# curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 89.3M  100 89.3M    0     0  28.4M      0  0:00:03  0:00:03 --:--:-- 28.4M
[root@lahdev ~]# sudo install minikube-linux-amd64 /usr/local/bin/minikube
[root@lahdev ~]# minikube start --force
* Centos 8.4.2105 의 minikube v1.32.0
! minikube skips various validations when --force is supplied; this may lead to unexpected behavior
* 자동적으로 docker 드라이버가 선택되었습니다. 다른 드라이버 목록: ssh, none
* The "docker" driver should not be used with root privileges. If you wish to continue as root, use --force.
* If you are running minikube within a VM, consider using --driver=none:
*   https://minikube.sigs.k8s.io/docs/reference/drivers/none/
* Using Docker driver with root privileges
* minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중
* 베이스 이미지를 다운받는 중 ...
* 쿠버네티스 v1.28.3 을 다운로드 중 ...
    > preloaded-images-k8s-v18-v1...:  403.35 MiB / 403.35 MiB  100.00% 31.03 M
    > gcr.io/k8s-minikube/kicbase...:  453.90 MiB / 453.90 MiB  100.00% 28.68 M
...중략
* 끝났습니다! kubectl이 "minikube" 클러스터와 "default" 네임스페이스를 기본적으로 사용하도록 구성되었습니다.
```

- kubectl 설치: [Kubectl Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
- 설치 확인

```bash
[root@lahdev ~]# kubectl get po -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS      AGE
kube-system   coredns-5dd5756b68-wx5pw           1/1     Running   0             48s
kube-system   etcd-minikube                      1/1     Running   0             61s
kube-system   kube-apiserver-minikube            1/1     Running   0             61s
kube-system   kube-controller-manager-minikube   1/1     Running   0             61s
kube-system   kube-proxy-vr8rj                   1/1     Running   0             48s
kube-system   kube-scheduler-minikube            1/1     Running   0             61s
kube-system   storage-provisioner                1/1     Running   1 (18s ago)   59s
[root@lahdev ~]#
```

### 5.4 여러 서버로 구성된 쿠버네티스 클러스터 설치

- 실제 설치 과정은 책을 참고 하는 것이 좋을 것 같음
- 주의 사항과 알아야할 것 들에 대해서만 정리

#### 사전 고려 사항

- 모든 서버의 시간이 NTP를 통해 동기화 되어있는지 확인
  - 네트워크가 불가능한 상황이라면 클러스터 중 한 노드에 ntp server를 구성하고 동기화하는 것도 방법이 될 것 같음 (주)
- 모든 서버의 맥(MAC) 주소가 다른지 확인
- 모든 서버의 하드웨어 스펙 확인: 2GB 메모리, 2 CPU
- 모든 서버에서 메모리 스왑(Swap)을 비활성화
  - 당연히 스왑을 꺼놓게 되면 out of memory(OOM Kill)이 발생할 수 있기 때문에 서버를 쿠버네티스 노드 외 용도로 사용하면 안 될 것 같음 (주)

```bash
[root@lahdev ~]# swapoff -a 
```

## 6. 쿠버네티스 시작하기

### 6.1 쿠버네티스를 시작하기 전에

- 모든 리소스는 "오브젝트"로 관리됨 (우리가 알고있는 추상적인 개념 그대로 이해하면 됨)
  - 스웜 모드에서 서비스라는 것을 사용해서 컨테이너의 묶음을 표현
  - 이 "서비스"도 컨테이너 리소스의 집합을 정의한 것 => 오브젝트
- 쿠버네티스에서의 예시
  - 컨테이너의 집합: Pods
  - 컨테이너 집합을 관리하는 컨트롤러: Replica Set
  - 사용자: Service Account
  - 노드: Node
  - 전부 오브젝트로 사용할 수 있음

- 오브젝트 종류 확인

```bash
[root@lahdev ~]# kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1
...중략
```

- 처음보는 오브젝트라면 explain으로 간단한 설명을 확인할 수 있음

```bash
[root@lahdev ~]#  kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
...중략
```

- kubectl을 통해 명령어로 실행할 수 있지만, 대부분 YAML 파일을 더 많이 사용
- 즉, 쿠버네티스를 잘 사용하는 것 => YAML 파일을 잘 작성하는 것

#### 쿠버네티스는 여러 컴포넌트로 구성됨

- 쿠버네티스 노드는 마스터와 워커로 구분
  - 마스터 노드는 클러스터를 관리하는 역할
  - 워커 노드는 애플리케이션 컨테이너가 생성
- 마스터 노드에서는 kube-apiserver, kube-controller-manager, kube-scheduler 등 여러 컴포넌트가 실행되고 있음

```bash
[root@lahdev ~]# docker ps
CONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS        PORTS                                                                                                                                  NAMES
90d961e23173   gcr.io/k8s-minikube/kicbase:v0.0.42   "/usr/local/bin/entr…"   24 hours ago    Up 24 hours   127.0.0.1:49157->22/tcp, 127.0.0.1:49156->2376/tcp, 127.0.0.1:49155->5000/tcp, 127.0.0.1:49154->8443/tcp, 127.0.0.1:49153->32443/tcp   minikube
e63ced19d9fe   004811815584                          "etcd --advertise-cl…"   5 weeks ago     Up 5 weeks                                                                                                                                           k8s_etcd_etcd-lahdev_kube-system_3841c848f0ac3f17e24aa760021629e8_33
aeba1f295a07   c7ab721dfdae                          "kube-controller-man…"   5 weeks ago     Up 5 weeks                                                                                                                                           k8s_kube-controller-manager_kube-controller-manager-lahdev_kube-system_e6b4c4a24f2fb437953a574c1b4449c7_6
42d66310ac9f   d4893b67e97f                          "kube-scheduler --au…"   5 weeks ago     Up 5 weeks                                                                                                                                           k8s_kube-scheduler_kube-scheduler-lahdev_kube-system_22204fa289db49368e406d46ad18bb8d_11
a1ba54b0b7ff   k8s.gcr.io/pause:3.5                  "/pause"                 5 weeks ago     Up 5 weeks                                                                                                                                           k8s_POD_etcd-lahdev_kube-system_3841c848f0ac3f17e24aa760021629e8_5
9e03bef7fac3   k8s.gcr.io/pause:3.5                  "/pause"                 5 weeks ago     Up 5 weeks                                                                                                                                           k8s_POD_kube-controller-manager-lahdev_kube-system_e6b4c4a24f2fb437953a574c1b4449c7_4
35440fbb7535   k8s.gcr.io/pause:3.5                  "/pause"                 5 weeks ago     Up 5 weeks                                                                                                                                           k8s_POD_kube-scheduler-lahdev_kube-system_22204fa289db49368e406d46ad18bb8d_4
f6007657359b   k8s.gcr.io/pause:3.5                  "/pause"                 5 weeks ago     Up 5 weeks                                                                                                                                           k8s_POD_kube-apiserver-lahdev_kube-system_c8dff736dc97abf00f5e3814bab21602_4
```

- 클러스터 구성을 위해 kubelet이라는 에이전트가 모든 노드에서 실행됨
- kubelet은 컨테이너의 생성, 삭제 뿐만 아니라 노드 간 통신 역할도 담당

### 6.2 포드(Pod): 컨테이너를 다루는 기본 단위

- 컨테이너 애플리케이션 구동을 위해 반드시 알아야 할 오브젝트: Pod, Replica Set, Service, Deployment

#### 6.2.1 포드 사용하기

- 컨테이너 애플리케이션의 기본 단위
- 1개 이상의 컨테이너로 구성된 컨테이너의 집합

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec:
  containers:
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
```

- apiVersion: 오브젝트의 API 버전 (사용하려는 오브젝트에 따라 달라질 수 있음)
- kind: 리소스 오브젝트 종류
- metadata: 라벨, 주석, 이름과 같은 부가 정보
- spec: 리소스를 생성하기 위한 자세한 정보

```bash
[root@lahdev study]# kubectl apply -f nginx-pod.yaml
pod/my-nginx-pod created
[root@lahdev study]# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   1/1     Running   0          25s
[root@lahdev study]# kubectl describe pods my-nginx-pod
Name:         my-nginx-pod
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2
Start Time:   Wed, 21 Feb 2024 18:22:35 +0900
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.0.3
IPs:
  IP:  10.244.0.3
...중략
```

- IP는 클러스터 내부에서만 접근할 수 있기 때문에 클러스터 내부로 들어간 후 요청해야함
- 다음 명령어로 임시 이미지를 통해 클러스터 내부로 들어갈 수 있는 임시 포드를 생성&접근 후 테스트가 가능

```bash
[root@lahdev study]# kubectl run -i --tty --rm debug --image=alicek106/ubuntu:curl --restart=Never bash
If you don't see a command prompt, try pressing enter.
root@debug:/#
root@debug:/# curl 10.244.0.3
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@debug:/#
```

- docker exec 명령어와 유사하게 kubectl exec 명령어로 포드의 컨테이너에 명령어를 전달할 수 있음
- 오브젝트 삭제시 delete -f 명령어로 삭제 가능 (pod 이름도 사용 가능)

```bash
[root@lahdev study]# kubectl exec -it my-nginx-pod bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@my-nginx-pod:/# ls /etc/nginx
conf.d  fastcgi_params  mime.types  modules  nginx.conf  scgi_params  uwsgi_params
root@my-nginx-pod:/# exit
[root@lahdev study]# kubectl delete -f nginx-pod.yaml
pod "my-nginx-pod" deleted
[root@lahdev study]#
```

#### 6.2.2 포드 vs. 도커 컨테이너

- 도커 컨테이너와 포드는 사용해보면 매우 비슷한데 왜 새로운 개념이 필요할까?
- 다음 결과에서 1/1이라는 항목에서 알 수 있듯이, 포드는 반드시 1개의 컨테이너로 구성되는 것이 아님

```bash
[root@lahdev study]# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   1/1     Running   0          25s
```

- 기존 YAML에 새 컨테이너를 추가하여 실행해보자

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec:
  containers:
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
  - name: ubuntu-sidecar-container
    image: alicek106/rr-test:curl
    command: ["tail"]
    args: ["-f", "/dev/null"]
```

```bash
[root@lahdev study]# kubectl apply -f nginx-pod.yaml
pod/my-nginx-pod created
[root@lahdev study]# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   2/2     Running   0          12s
[root@lahdev study]#
[root@lahdev study]# kubectl exec -it my-nginx-pod -c ubuntu-sidecar-container bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@my-nginx-pod:/# curl localhost
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@my-nginx-pod:/#
```

- ubuntu-sidecar-container 컨테이너에 들어가서 로컬호스트로 HTTP 요청을 전송하면 Nginx 서버의 응답이 도착하는 것을 확인할 수 있음
- 즉, 같은 리눅스 네임스페이스를 공유하기 때문에 가능.

![같은 포드 내의 컨테이너는 네트워크 네임스페이스를 공유](./assets/img3.png?raw=true)

- 공유하는 리눅스 네임스페이스는 네트워크 환경만 있는 것은 아니지만, 나중에 다룸

#### 6.2.3 완전한 애플리케이션으로서의 포드

- 하나의 포드는 하나의 완전한 애플리케이션
- Nginx 컨테이너는 그 자체만으로도 완전한 애플리케이션. 따라서 하나의 포드에 2개의 Nginx 컨테이너가 정의되는 것은 바람직하지 않음
- 만약 부가적인 기능이 필요하다면?
  - 설정을 재구성해주는 reloader 프로세스
  - 로그를 수집하는 프로세스 등
  - 이러한 프로세스는 Nginx 컨테이너와 함께 실행되어야 함
- 포드의 주 컨테이너는 Nginx
- 기능 확장을 위한 추가 컨테이너를 포함시킬 수 있음: 사이드카 컨테이너

### 6.3 레플리카셋(Replica Set): 일정 개수의 포드를 유지하는 컨트롤러

#### 6.3.1 레플리카셋을 사용하는 이유

- 포드만 정의해서 생성하면 포드의 LifeCycle은 어떻게 될까?
  - kubectl delete 명령어로 포드를 삭제하면 포드의 컨테이너와 포드가 전부 삭제됨
  - 즉, 포드만 정의하는 경우 포드는 사용자에 의해서만 관리됨
  - 스웜 모드에서 다뤘던 것처럼 여러 개의 동일한 컨테이너를 생성하고 분배하고 싶다면?
- 포드를 여러개 생성해 분배할 수 있어야 함

![같은 포드 내의 컨테이너는 네트워크 네임스페이스를 공유](./assets/img4.png?raw=true)

- YAML 파일에 ---를 구분자로 사용하여 여러 개의 리소스를 정의할 수 있으나 매우 비효율
- 따라서 포드만 정의해서 사용하지 않고, 레플리카셋을 통해 보완

##### 레플리카셋 역할

- 정해진 수의 동일한 포드가 항상 실행되도록 관리
- 노드 장애 등의 이유로 포드를 사용할 수 없다면 다른 노드에서 포드를 다시 생성

#### 6.3.2 레플리카셋 사용하기

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-nginx-pods-label
  template:
    metadata:
      name: my-nginx-pod
      labels: 
        app: my-nginx-pods-label
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

- selector 영역이 레플리카셋을 정의하는 부분
- template 영역이 포드를 정의하는 부분

```bash
[root@lahdev study]# kubectl apply -f replicaset-nginx.yaml
replicaset.apps/replicaset-nginx created
[root@lahdev study]# kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
replicaset-nginx-4k6hm   1/1     Running   0          58s
replicaset-nginx-g4j64   1/1     Running   0          58s
replicaset-nginx-lbnrt   1/1     Running   0          58s
[root@lahdev study]#
```

- 이렇게 생성된 포드는 레플리카셋에 의해서 생성된 포드로, 레플리카셋을 수정하거나 삭제하면 포드에 반영됨
- 만약 yaml파일의 replicas값을 4로 변경한 후 apply할 경우 4개로 증가 (하나가 추가됨)

```bash
[root@lahdev study]# kubectl apply -f replicaset-nginx-4pods.yaml
replicaset.apps/replicaset-nginx configured
[root@lahdev study]# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
replicaset-nginx-4k6hm   1/1     Running   0          2m49s
replicaset-nginx-8gwch   1/1     Running   0          8s
replicaset-nginx-g4j64   1/1     Running   0          2m49s
replicaset-nginx-lbnrt   1/1     Running   0          2m49s
```

- 다음 명령어로 삭제할 수 있음

```bash
[root@lahdev study]# kubectl delete rs replicaset-nginx
replicaset.apps "replicaset-nginx" deleted
[root@lahdev study]# kubectl get pods
No resources found in default namespace.
[root@lahdev study]#
```

#### 6.3.3 레플리카셋의 동작 원리

- 레플리카셋과 포드는 연결되어있지 않음
- 두 개념은 느슨한 연결(loosely coupled)을 유지함
- 이는 포드와 레플리카셋의 정의 중 라벨 셀렉터(Label Selector)을 통해 이뤄짐

```yaml
...
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-nginx-pods-label
  template:
    metadata:
      name: my-nginx-pod
      labels:
        app: my-nginx-pods-label
...
```

- 레플리카셋은 spec.selector.matchLabel에 정의된 라벨을 통해 생성해야하는 포드를 찾음
- 해당 라벨을 가진 포드가 존재하는지 확인한 후에 replica 수에 맞춰 생성하거나 동작하지 않거나 함
- 따라서 해당 라벨을 가진 포드를 레플리카셋을 통하지 않고 수동으로 생성하거나 삭제해도 동일하게 관리됨
  - 하지만 이는 동작과정을 이해하기 위해 설명한 내용이지, 바람직한 방법은 아님

![라벨 셀렉터와 일치하는 포드만 관리 대상으로 간주](./assets/img5.png?raw=true)

#### 6.3.4 레플리케이션 컨트롤러 vs. 레플리카셋

- 과거 버전의 쿠버네티스에서는 레플리카셋이 아닌 레플리케이션 컨트롤러(Replication Controller)라는 오브젝트를 통해 포드수를 유지했음 (deprecated)
- 자세히 알 필요는 없지만, 다른 점 중 하나는 레플리카셋이 "표현식" 기반의 라벨 셀렉터를 사용할 수 있다는 점

```yaml
...
selector:
  matchExpressions:
    - key: app
      values:
        - my-nginx-pods-label
        - your-nginx-pods-label
      operator: In
...
```

- 위 예시는 app:my-nginx-pods-label과 app:your-nginx-pods-label 라벨을 가진 포드를 관리할 수 있음

### 6.4 디플로이먼트(Deployment): 레플리카셋, 포드의 배포를 관리

#### 6.4.1 디플로이먼트 사용하기

- 실제 운영에서 레플리카셋을 YAML파일에 정의해서 사용하는 경우는 거의 없음
- 대부분 레플리카셋과 포드의 정보를 정의하는 디플로이먼트 오브젝트를 YAML에 정의해서 사용
- 디플로이먼트는 레플리카셋의 상위 오브젝트이기 때문에 디플로이먼트에 대응하는 레플리카셋도 함께 생성됨

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-nginx
  template:
    metadata:
      name: my-nginx-pod
      labels:
        app: my-nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.10
        ports:
        - containerPort: 80
```

- kind 항목만 Deployment로 바뀐것 처럼 보임

```bash
[root@lahdev study]# kubectl apply -f deployment-nginx.yaml
deployment.apps/my-nginx-deployment created
[root@lahdev study]# kubectl get deployment
NAME                     READY   UP-TO-DATE    AVAILABLE   AGE
my-nginx-deployment      3/3     3             3           39s
[root@lahdev study]# kubectl get replicasets
NAME                             DESIRED CURRENT  READY    AGE
my-nginx-deployment-85d657c94d   3       3        3        5m31s
[root@lahdev study]# kubectl get pods
NAME                                  READY  STATUS    RESTARTS  AGE
my-nginx-deployment-85d657c94d-84trw  1/1    Running   0         4m49s
my-nginx-deployment-85d657c94d-14qpz  1/1    Running   0         4m49s
my-nginx-deployment-85d657c94d-ttj74  1/1    Running   0         4m49s
[root@lahdev study]# kubectl delete deploy my-nginx-deployment
```

- 85d657c94d라는 해시값이 포함되어있는데, 이는 포드를 정의하는 템플릿으로부터 생성된 값

#### 6.4.2 디플로이먼트를 사용하는 이유

- 애플리케이션의 업데이트와 배포를 편하게 하기 위해서 사용
- 레플리카셋의 변경 사항을 저장하는 리비전을 남겨 롤백이 가능
- 무중단 서비스를 위해 포드의 롤링 업데이트를 지정할 수 있음

```bash
[root@lahdev study]# kubectl apply -f deployment-nginx.yaml --record
```

- --record 옵션을 지정하여 디플로이먼트 생성
- 애플리케이션의 버전이 업데이트되어 포드의 이미지를 변경해야 한다고 가정
  - kubectl set image 명령어를 통해 이미지를 변경할 수 있음

```bash
[root@lahdev study]# kubectl set image deployment my-nginx-deployment nginx=nginx:1.11 --record
[root@lahdev study]# kubectl get pods
NAME                                  READY  STATUS    RESTARTS  AGE
my-nginx-deployment-67fff69f59-84trw  1/1    Running   0         4m49s
my-nginx-deployment-67fff69f59-14qpz  1/1    Running   0         4m49s
my-nginx-deployment-67fff69f59-ttj74  1/1    Running   0         4m49s
[root@lahdev study]# kubectl get replicasets
NAME                             DESIRED CURRENT  READY    AGE
my-nginx-deployment-67fff69f59   3       3        3        61s
my-nginx-deployment-85d657c94d   0       0        0        5m31s
```

- 85d657c94d 해시값은 처음 생성했던 레플리카셋으로, --record 옵션을 통해 매 지점의 리비전을 저장하고 있음
- rollout 명령어를 통해 자세히 히스토리를 확인할 수 있음

```bash
[root@lahdev study]# kubectl rollout history deployment my-nginx-deployment
deployment.apps/my-nginx-deployment
REVISION CHANGE-CAUSE
1        kubectl apply --filename=deployment-nginx.yaml --record=true
2        kubectl set image deployment my-nginx-deployment nginx=nginx:1.11 --record=true
```

- 이는 --to-revision에 리비전의 번호를 입력하여 롤백을 할 수 있음

```bash
[root@lahdev study]# kubectl rollout undo deployment my-nginx-deployment --to-revision=1
deployment.apps/my-nginx-deployment rolled back
[root@lahdev study]# kubectl get replicasets
NAME                             DESIRED CURRENT  READY    AGE
my-nginx-deployment-67fff69f59   0       0        0        2m61s
my-nginx-deployment-85d657c94d   3       3        3        6m31s
```

- 이는 describe 명령어를 통해 자세히 확인할 수 있음

![디플로이먼트를 통한 레플리카셋의 버전 관리](./assets/img6.png?raw=true)

### 6.5 서비스(Service): 포드를 연결하고 외부에 노출

- 클러스터 내부에 접근한 이후에 포드에 접근할 수 있었으나, 외부에서 접근할 수 없음
- 여러 개의 디플로이먼트를 하나의 애플리케이션으로 연동하려면 서로를 Discovery할 수 있는 방법이 필요
- 도커 예시

```bash
[root@lahdev study]# docker run -d --name myapp -p 80:80 nginx:latest
[root@lahdev study]# docker run -itd --name myapp2 --link myapp:nginx ubuntu:16.04

[root@lahdev study]# docker network create mynetwork
[root@lahdev study]# docker run mycontainer --network mynetwork --net-alias mycon ubuntu:16.04
```

- 디플로이먼트의 YAML을 확인해보면 containerPort항목을 정의했지만, 이 포트는 외부에 노출되지 않음
- 따라서 서비스(Service)라는 별도의 오브젝트를 생성해야 함

**서비스의 핵심 기능**

- 여러 개의 포드에 쉽게 접근할 수 있도록 고유한 도메인 이름을 부여
- 여러 개의 포드에 접근할 때, 요청을 분산(로드 밸런싱)

- 쿠버네티스를 설치할 때 어떤 네트워크 플러그인을 사용하느냐에 따라 기능/성능에 차이가 있음 (기본 설치되는 calico 기준ㄴ, 핵심 기능 사용에 별 문제 없음)

#### 6.5.1 서비스(Service)의 종류

- 예시를 위한 디플로이먼트 생성: 컨테이너(포드)의 호스트 이름을 반환하는 웹 서버

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostname-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      name: my-webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: my-webserver
        image: alicek106/rr-test:echo-hostname
        ports:
        - containerPort: 80
```

**서비스 타입 3가지**

- ClusterIP 타입: 쿠버네티스 내부에서만 포드들에 접근할 때 사용. 외부에 포드를 노출하지 않을 때 사용
- NodePort 타입: 포드에 접근할 수 있는 포트를 클러스터의 모든 노드에 동일하게 개방. 포트는 기본적으로 랜덤으로 정해지지만 지정할 수 있음
- LoadBalancer 타입: GCP, AWS와 같은 클라우드 플랫폼에서 제공하는 로드 밸런서 타입. NodePort와 마찬가지로 외부에서 포드에 접근할 수 있음

#### 6.5.2 ClusterIP 타입의 서비스 - 내부에서만 포드에 접근하기

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-clusterip
spec:
  ports:
    - name: web-port
      port: 8080
      targetPort: 80
  selector:
    app: webserver
  type: ClusterIP
```

- spec.selector 항목을 통해 이 서비스에서 어떠한 라벨을 가지ㅏ는 포드에 접근할 수 있게 만들 것인지 결정
- app:webserver라는 라벨을 가지는 포드들의 집합에 접근할 수 있는 서비스를 생성
- 앞서 생성한 hostname-deployment의 경우 이 라벨이 설정되어있어 이 서비스에 의해 접근이 가능하게 됨
- spec.ports.port는 서비스의 IP에 접근할 때 사용할 포트이고, spec.ports.targetPort는 포드의 containerPort 값

```bash
[root@lahdev study]# kubectl apply -f hostname-svc-clusterip.yaml
service/hostname-svc-clusterip created
[root@lahdev study]# kubectl get services
NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE 
hostname-svc-clusterip   ClusterIP  10.101.98.33   <none>        8080/TCP   3s
kubernetes               ClusterIP  10.96.0.1      <none>        443/TCP    18d

[root@lahdev study]# kubectl run -i - - t t y --rm debug --image=alicek106/ubuntu:curl --restart=Never -- bash
If you dont see a command prompt, try pressing enter.
root®debug:/# curl 10.101.98.33:8080 --silent | grep Hello
‹p›Hello, hostname-deployment-c648cf85f-cs8zt</p> </blockquote>
rootedebug:/# curl 10.101.98.33:8080 --silent | grep Hello
<p›Hello, hostname-deployment-c648cf85f-z8696</p> </blockquote>
root®debug:/# curl 10.101.98.33:8080 --silent | grep Hello
<p›Hello, hostname-deployment-c648cf85f-nmaq6</p> </blockquote>
```

- 8080 포트로 호출하게 되면 로드밸런싱 되면서 각 포드의 80포트에 요청이 되어 결과를 확인할 수 있음

![ClusteIP 타입의 서비스를 통해 여러 포드에 접근](./assets/img7.png?raw=true)

- 서비스의 라벨 셀렉터와 포드의 라벨이 매칭돼 연결되면 쿠버네티스는 자동으로 엔드포인트라는 오브젝트를 별도로 생성
- 이는 자동으로 생성되기 때문에 자세히 알 필요는 없


#### 6.5.3 NodePort 타입의 서비스 - 서비스를 이용해 포드를 외부에 노출하기

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-nodeport
spec:
  ports:
    - name: web-port
      port: 8080
      targetPort: 80
  selector:
    app: webserver
  type: NodePort
```

```bash
[root@lahdev study]# kubectl apply -f hostname-svc-nodeport.yaml
service/hostname-svc-nodeport created
[root@lahdev study]# kubectl get services
NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE 
hostname-svc-nodeport    NodePort   10.110.9.251   <none>        8080:31514/TCP  3s
kubernetes               ClusterIP  10.96.0.1      <none>        443/TCP         18d

[root@lahdev study]# curl 10.43.0.30:31514 --silent | grep Hello
‹p›Hello, hostname-deployment-c648cf85f-cs8zt</p> </blockquote>
[root@lahdev study]# curl 10.43.0.31:31514 --silent | grep Hello
<p›Hello, hostname-deployment-c648cf85f-z8696</p> </blockquote>
[root@lahdev study]# curl 10.43.0.32:31514 --silent | grep Hello
<p›Hello, hostname-deployment-c648cf85f-nmaq6</p> </blockquote>
```

- 단 GKE에서 사용하는 경우 gcloud 방화벽 규칙을 추가해야함
- 포트는 30000 ~ 32768 포트 중에 랜덤으로 선택되지만, yaml에 명시할 수 있음 (nodePort)

```yaml
...
spec:
  ports:
    - name: web-port
      port: 8080
      targetPort: 80
      nodePort: 31000
...
```

- NodePort 타입의 서비스도 CLUSTER-IP가 부여되기 때문에 내부, 외부 네트워크 양쪽 전부 접근할 수 있음

![NodePort 타입 서비스의 트래픽 흐름](./assets/img8.png?raw=true)

- 실제 운영 환경에서 NodePort로 서비스를 외부에 제공하는 경우는 많지 않음
- SSL 인증서 적용, 라우팅 등을 적용해야하기 때문에 NodePort에 80, 443을 설정하지 않음
- 인그레스(Ingress)라고 부르는 오브젝트에서 간접적으로 사용되는 경우가 많음
- LoadBalancer와 NodePort를 합치면 인그레스 오브젝트를 사용할 수 있음 (8장에서 다룸)

**(주) 회사에서 쓰고 있는 Ingress 예시**

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: "gce"
    kubernetes.io/ingress.global-static-ip-name: vidius-ingress-address
    networking.gke.io/managed-certificates: vidius-certificate
    networking.gke.io/v1beta1.FrontendConfig: "vidius-frontend-config-redirect"
  name: vidius-ingress
  namespace: vidius
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: vidius-backend-service
          servicePort: 8081
        path: /api
        pathType: Prefix
      - backend:
          serviceName: vidius-backend-service
          servicePort: 8081
        path: /ckeditor
        pathType: Prefix
      - backend:
          serviceName: vidius-backend-service
          servicePort: 8081
        path: /baton
        pathType: Prefix
      - backend:
          serviceName: vidius-backend-service
          servicePort: 8081
        path: /admin
        pathType: Prefix
      - backend:
          serviceName: vidius-media-bucket-service
          servicePort: 8082
        path: /static
        pathType: Prefix
      - backend:
          serviceName: vidius-media-bucket-service
          servicePort: 8082
        path: /media
        pathType: Prefix
      - backend:
          serviceName: vidius-frontend-service
          servicePort: 8080
        path: /
        pathType: Prefix
```

#### 6.5.4 클라우드 플랫폼의 로드 밸런서와 연동하기 - LoadBalancer 타입의 서비스

- NodePort를 사용할 때는 각 노드의 IP를 알아야 하지만, LoadBalancer 타입은 클라우드 플랫폼으로부터 도메인 이름과 IP를 할당받기 때문에 포드 접근이 쉬움
- 로드 밸런서를 동적으로 생성하는 기능을 제공하는 환경에서만 사용 가능 (AWS, GCP)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-nlb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  ports:
    - name: web-port
      port: 80
      targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
```

- 쿠버네티스에서 주석(annotations)은 라벨처럼 추가적인 정보를 나타내지만, 특정 용도로 미리 정의된 주석이 있음
- 위 예시는 로드 밸런서 유형을 클래식이 아닌 Network Load Balancer를 설정한다는 의미
- (주) NodePort와 동일하지만 클라우드로부터 도메인 이름을 받아 접근이 가능한 점만 다름

![LoadBalancer 타입 서비스의 트래픽 흐름](./assets/img9.png?raw=true)

- 온프레미스 환경에서 LoadBalancer 타입을 사용하기 위해서는 MetaLB와 같은 별도의 로드 밸런서를 구성하고 연결해야함

#### 6.5.5 트래픽의 분배를 결정하는 서비스 속성 : externalTrafficPolicy

- 요청이 들어온 노드와 실행하는 포드가 위치한 노드가 다를 수 있음

![노드A로 와서 포드 b에서 처리](./assets/img10.png?raw=true)

- 기본적으로 externalTrafficPolicy값이 Cluster로 설정되어 있음

```bash
[root@lahdev ~]# kubectl get svc hostname-svc-nodeport -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
...
  externalTrafficPolicy: Cluster
...
```

- 이 값을 spec에 정의하여 수정할 수 있음

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-lb-local
spec:
  externalTrafficPolicy: Local
  ports:
    - name: web-port
      port: 80
      targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
```

- 다만 자원 활용률(Utilization)은 바람직하지 않을 수 있음

![Utilization 비효율](./assets/img11.png?raw=true)

#### 6.5.6 요청을 외부로 리다이렉트하는 서비스: ExternalName

- 쿠버네티스를 외부 시스템과 연동할 때 사용
- 서비스가 외부 도메인을 가리키도록 설정할 수 있음

```yaml
apiVersion: v1
kind: Service
metadata:
  name: externalname-svc
spec:
  type: ExternalName
  externalName: my.database.com
```

- externalname-svc로 요청을 보내면 이 요청들은 my.database.com으로 접근하게 됨
- 쿠버네티스와 별개로 레거시 시스템에 연동해야할 때 유용하게 사용할 수 있음
